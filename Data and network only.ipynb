{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('train.json',  lines = True).drop(['index'], axis = 1)\n",
    "test_df = pd.read_json('test.json', lines = True).drop(['index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2int = {x:i for i, x in enumerate('().ACGUBEHIMSXZ')}\n",
    "\n",
    "target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n",
    "\n",
    "def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n",
    "    return np.transpose(\n",
    "        np.array(\n",
    "            df[cols]\n",
    "            .applymap(lambda seq: [token2int[x] for x in seq])\n",
    "            .values\n",
    "            .tolist()\n",
    "        ),\n",
    "        (0, 2, 1)\n",
    "    )\n",
    "\n",
    "train_inputs = preprocess_inputs(train_df.loc[train_df.SN_filter == 1])\n",
    "train_labels = np.array(train_df.loc[train_df.SN_filter == 1][target_cols].values.tolist()).transpose((0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1430, 107, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 123\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_inputs, train_labels, test_size = 0.1, random_state = seed)\n",
    "\n",
    "print(np.shape(X_train)) #1430 samples de s√©quences de 107 de long et de 3=sequence,structure, predicted loop type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as L\n",
    "def gru_layer(hidden_dim, dropout):\n",
    "    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True))\n",
    "\n",
    "def build_model(seq_len=107, pred_len=68, dropout=0.5, embed_dim=75, hidden_dim=128):\n",
    "    inputs = L.Input(shape=(seq_len, 14))\n",
    "\n",
    "    embed = L.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs)\n",
    "    reshaped = tf.reshape(\n",
    "        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n",
    "\n",
    "    hidden = gru_layer(hidden_dim, dropout)(reshaped)\n",
    "    hidden = gru_layer(hidden_dim, dropout)(hidden)\n",
    "    hidden = gru_layer(hidden_dim, dropout)(hidden)\n",
    "    \n",
    "    # Since we are only making predictions on the first part of each sequence, we have\n",
    "    # to truncate it\n",
    "    truncated = hidden[:, :pred_len]\n",
    "    \n",
    "    out = L.Dense(3, activation='linear')(truncated)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "\n",
    "    model.compile(tf.keras.optimizers.Adam(), loss='mse',  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(seq_len=107, pred_len=68, dropout=0.5, embed_dim=75, hidden_dim=128):\n",
    "    inputs = L.Input(shape=(seq_len, 3))\n",
    "\n",
    "    embed = L.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs)\n",
    "    reshaped = tf.reshape(\n",
    "        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n",
    "\n",
    "    hidden = gru_layer(hidden_dim, dropout)(reshaped)\n",
    "    hidden = gru_layer(hidden_dim, dropout)(hidden)\n",
    "    hidden = gru_layer(hidden_dim, dropout)(hidden)\n",
    "    \n",
    "    # Since we are only making predictions on the first part of each sequence, we have\n",
    "    # to truncate it\n",
    "    truncated = hidden[:, :pred_len]\n",
    "    \n",
    "    out = L.Dense(5, activation='linear')(truncated)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "\n",
    "    model.compile(tf.keras.optimizers.Adam(), loss='mse',  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_window_training(base_index, n_bases,X_train, Y_train, X_test, Y_test):\n",
    "   \n",
    "    start_index=int(i-(window_size-1)/2)\n",
    "    stop_index=int(i+(window_size-1)/2 +1) #not included\n",
    "    full_array=[]\n",
    "    for sample_index in range(len(X_train)):\n",
    "        sample_array=X_train[sample_index]\n",
    "\n",
    "        if start_index<0:\n",
    "            out_array=np.array([[14]*len(sample_array[0])]*(-int(start_index)))\n",
    "            studied_cut_array=sample_array[0:stop_index]\n",
    "            studied_array=np.concatenate((out_array,studied_cut_array))\n",
    "\n",
    "        elif stop_index>68:\n",
    "            out_array=np.array([[14]*len(sample_array[0])]*(int(stop_index-68)))\n",
    "            studied_cut_array=sample_array[start_index:68]\n",
    "            studied_array=np.concatenate((studied_cut_array,out_array))\n",
    "\n",
    "        else :\n",
    "            studied_array=sample_array[start_index:stop_index]\n",
    "        full_array.append(studied_array)\n",
    "    \n",
    "    full_array_test=[]\n",
    "    for sample_index in range(len(X_test)):\n",
    "        sample_array=X_test[sample_index]\n",
    "\n",
    "        if start_index<0:\n",
    "            out_array=np.array([[14]*len(sample_array[0])]*(-int(start_index)))\n",
    "            studied_cut_array=sample_array[0:stop_index]\n",
    "            studied_array=np.concatenate((out_array,studied_cut_array))\n",
    "\n",
    "        elif stop_index>68:\n",
    "            out_array=np.array([[14]*len(sample_array[0])]*(int(stop_index-68)))\n",
    "            studied_cut_array=sample_array[start_index:68]\n",
    "            studied_array=np.concatenate((studied_cut_array,out_array))\n",
    "\n",
    "        else :\n",
    "            studied_array=sample_array[start_index:stop_index]\n",
    "        full_array_test.append(studied_array)\n",
    "\n",
    "    X_train_window=np.array(full_array)\n",
    "    Y_train_window=Y_train[:,i,:]\n",
    "    X_test_window=np.array(full_array_test)\n",
    "    Y_test_window=Y_test[:,i,:]\n",
    "    \n",
    "    return X_train_window, Y_train_window, X_test_window, Y_test_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 21, 3)]           0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 21, 3, 15)         225       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Reshape_10 (Tens [(None, 21, 45)]          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, 21, 256)           134400    \n",
      "_________________________________________________________________\n",
      "bidirectional_28 (Bidirectio (None, 21, 256)           296448    \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, 21, 256)           296448    \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice_9  [(None, 21, 256)]         0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 21, 5)             1285      \n",
      "=================================================================\n",
      "Total params: 728,806\n",
      "Trainable params: 728,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1430, 21, 3)\n",
      "(1430, 21, 5)\n",
      "(159, 21, 3)\n",
      "(159, 21, 5)\n",
      "Epoch 1/10\n",
      "45/45 - 6s - loss: 0.2898 - accuracy: 0.9598 - val_loss: 0.1329 - val_accuracy: 0.9748\n",
      "Epoch 2/10\n",
      "45/45 - 5s - loss: 0.1510 - accuracy: 0.9776 - val_loss: 0.1173 - val_accuracy: 0.9748\n",
      "Epoch 3/10\n",
      "45/45 - 5s - loss: 0.1467 - accuracy: 0.9776 - val_loss: 0.1064 - val_accuracy: 0.9748\n",
      "Epoch 4/10\n",
      "45/45 - 5s - loss: 0.1438 - accuracy: 0.9776 - val_loss: 0.1050 - val_accuracy: 0.9748\n",
      "Epoch 5/10\n",
      "45/45 - 5s - loss: 0.1388 - accuracy: 0.9776 - val_loss: 0.1066 - val_accuracy: 0.9748\n",
      "Epoch 6/10\n",
      "45/45 - 5s - loss: 0.1389 - accuracy: 0.9776 - val_loss: 0.1120 - val_accuracy: 0.9748\n",
      "Epoch 7/10\n",
      "45/45 - 5s - loss: 0.1367 - accuracy: 0.9776 - val_loss: 0.1048 - val_accuracy: 0.9748\n",
      "Epoch 8/10\n",
      "45/45 - 5s - loss: 0.1352 - accuracy: 0.9776 - val_loss: 0.1078 - val_accuracy: 0.9748\n",
      "Epoch 9/10\n",
      "45/45 - 5s - loss: 0.1383 - accuracy: 0.9776 - val_loss: 0.1040 - val_accuracy: 0.9748\n",
      "Epoch 10/10\n",
      "45/45 - 5s - loss: 0.1359 - accuracy: 0.9776 - val_loss: 0.1057 - val_accuracy: 0.9748\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 21, 3) for input Tensor(\"input_11:0\", shape=(None, 21, 3), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Input to reshape is a tensor with 45 values, but the requested shape requires a multiple of 945\n\t [[node functional_19/tf_op_layer_Reshape_10/Reshape_10 (defined at <ipython-input-33-2904ca280399>:47) ]] [Op:__inference_predict_function_75380]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2904ca280399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_to_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mseq_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_to_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mbase_predict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mY_predicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MyJupy/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/.conda/envs/MyJupy/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MyJupy/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MyJupy/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    844\u001b[0m               *args, **kwds)\n\u001b[1;32m    845\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MyJupy/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.conda/envs/MyJupy/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.conda/envs/MyJupy/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MyJupy/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Input to reshape is a tensor with 45 values, but the requested shape requires a multiple of 945\n\t [[node functional_19/tf_op_layer_Reshape_10/Reshape_10 (defined at <ipython-input-33-2904ca280399>:47) ]] [Op:__inference_predict_function_75380]\n\nFunction call stack:\npredict_function\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#mode fen√™tr√©:\n",
    "window_size=21 #doit √™tre un nombre impair\n",
    "X_train=X_train[:,:68,:]\n",
    "X_test=X_test[:,:68,:]\n",
    "\n",
    "X_to_predict = preprocess_inputs(train_df)[:,:68,:]\n",
    "\n",
    "\n",
    "for i in range(68): #on parcourt toute la s√©quence\n",
    "\n",
    "    X_train_window, Y_train_window, X_test_window, Y_test_window = prepare_window_training(i, 68, \n",
    "                                                                                   X_train, Y_train, \n",
    "                                                                                   X_test, Y_test)\n",
    "    #ici il faut faire le fit sur le full_array de dimensions 2160*21*14=nombredes√©quences*window_size*nombredeparam√®tresd'input\n",
    "    #ensuite on lance le mod√®le sur les vraies donn√©es, \n",
    "    #et on stocke le r√©sultat [a,b,c] dans l'item i une liste du type [[1, 0, 0], [0, 1, 0.3], [0.4, 0.7, 1], ...], de dimensions 68*3\n",
    "    \n",
    "    model = build_model(seq_len=21, embed_dim=len(token2int))\n",
    "    model.summary()\n",
    "    Y_train_window=np.array([np.tile(Y_train_window[i],(21,1)) for i in range(len(Y_train_window))])\n",
    "    Y_test_window=np.array([np.tile(Y_test_window[i],(21,1)) for i in range(len(Y_test_window))])\n",
    "\n",
    "    print(np.shape(X_train_window))\n",
    "    print(np.shape(Y_train_window))\n",
    "    print(np.shape(X_test_window))\n",
    "    print(np.shape(Y_test_window))\n",
    "    \n",
    "    history = model.fit(\n",
    "    X_train_window, \n",
    "    Y_train_window,\n",
    "    validation_data=(X_test_window, Y_test_window),\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(patience=5),\n",
    "        tf.keras.callbacks.ModelCheckpoint('model.h5')\n",
    "    ]\n",
    "    )\n",
    "    plt.subplot()\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.legend(['val_loss', 'loss'], loc='upper right')\n",
    "    \n",
    "# la pr√©diction qui suit ne marche pas encore \n",
    "\n",
    "#     for seq in range(len(X_to_predict)) :\n",
    "#         base_predict=model.predict(X_test_window[0,:,:])\n",
    "#         Y_predicted[seq, i, :]=np.mean(base_predict, axis=0)\n",
    "\n",
    "#         print(Y_predicted[seq,i,:])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la pr√©paration des donn√©es, (mode fen√™tr√©), au lieu de mettre des -1 (ou des 14 en fait) √† la fin de la s√©quence on peut mettre les vraies bases et structures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
